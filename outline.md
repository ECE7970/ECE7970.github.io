
### [Topic 01: High Dimensional Space](#1)

### [Topic 02: Best-Fit Subspace and Singular Value Decomposition(SVD)](#2)

### [Topic 03: Random Matrix Theory](#3)

### [Topic 04: Deep Learning](#4)

### [Topic 05: Random Matrix Theory for Deep Learning](#5)

### [Topic 06:Tensor for Deep learning](#6)

---
 <h3 id="1">
 Topic 01: High Dimensional Space
 </h3>
 
## Lectrue 1 
### Big data:
- opportunities and challenges
- High dimensional statistics 
- Smart grid
- Cognitive radio network
- Cognitive radar
- Cognitive networked sensing

### Algorithms:
- Random forest
- SVM
- Decision tree
- PCA
- Kernel PCA
- Foundation
   * Deep learning: Classification and Prediction
   * Supervised learning and Unsupervised learning


## Lectrue 2 
- High dimension distance
- Supervised learning 
- The law of large number 
- Markov’s inequality
 -Chebyshev’s inequality

 
 
## Lecture 3 
- Semi-circle distribution
- High dimensional space
- PCA and SVD
- Chebyshev’s Inequality
- Distance of variables
- Single ring law
- Tail Bounds theorem
- Geometry of high dimensions

## Lecture 4  
- Semi-circle distribution
- CDF and PDF
- Random projection 
- Gaussian annulus theorem 



Reference：  
* [Spectrum Analysis of Large Random Matrices A Brief Introduction](/assets/ppt/SJTU_EE_PhD_Course_Notes_2017.pdf) by Robert C. Qiu
  
## Lecture 5  
- MP-Law
- Diagonal entries 
- Single-Ring distribution  
  * Outlier detection and anomaly  detection 
- Random Projection and Johnson–Lindenstrauss lemma


---

 <h3 id="2">
 Topic 02: Best-Fit Subspace and Singular Value Decomposition(SVD)
 </h3>
 
## Lecture 6
- Singular Values 
- Singular Vector Decomposition(SVD)

## Lecture 7
- Best Rank-k Approximations
- Power method for Singular Value Decomposition
- Singular Vectors and Eigenvectors
- Applications of Singular Value Decomposition
- Principal Component Analysis
- Clustering a Mixture of Spherical Gaussians

## Lecture 8
- Johnson-Lindertrauss lemma
- Dvoretzky-Milman’s therem     
- Chebyshev’s Inequality
- Lindeberg-Lévy Central Limit Theorem
- Concentration sum of independent
- Random vector in high dimension
- Kernel function: kernel PCA

## Lecture 9  
- Kernel function
- Gaussian function
- covariance matrix

Text Book(Lectrue 1 to Lectrue 9):
- [Foundation of Data Science](https://www.cs.cornell.edu/jeh/book.pdf)

---

 <h3 id="3">
 Topic 03: Random Matrix Theory
 </h3>
 
## Lecture 10
- Random matrix theory and big data
- Concentration without independence
- Quadratic forms, summarization and contraction
- Covariance estimation for general distributions
- Sub-Gaussian increments of the random matrix process
- Sample complexity
- Tail bound
- spectral clustering
- Gaussian mixture model

Paper:
- [Sketching as a Tool for Numerical Linear Algebra](http://researcher.watson.ibm.com/researcher/files/us-dpwoodru/wNow.pdf)
Textbook:
[High-Dimensional Probalbility](http://www-personal.umich.edu/~romanv/papers/HDP-book/HDP-book.pdf)



 
## Lecture 11
- Introduction to theorem Non –Asymptotic analysis of random metrics
- Concentration of Lipchitz function on the sphere
- Lipchitz functions
- Concentration via isoperimetric inequalities
- Gaussian concentration
- Talargand’s concentration inequality
- Random projection
- Grassmannian
- Correlation coefficient
- Conditional density

---
 <h3 id="4">
 Topic 04: Deep Learning
 </h3>
 
## Lecture 12
- Deep learning
- Why is speech recognition hard?
- The idea about deep learning
- The one  learning algorithm hypothesis
- Neural network
- Deep learning trend
- Bigger is better
- Application
- AI will transform the internet

Paper:  
- [Deep Learning](http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf) by Andrew Ng

## Lecture 13
- Softmax function
- Sigmoid Function 
- Rectified Linear Unit Function



Paper:  
- [Deep Learning Tutorial](http://speech.ee.ntu.edu.tw/~tlkagk/slide/Deep%20Learning%20Tutorial%20Complete%20(v3)) by Hung-yi Lee
- [Supervised Sequence Labelling with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/preprint.pdf)
- [Deep Learning](./assets/paper/deep-learning-nature.pdf) in nature journal
- [Optimal approximation of piecewise smooth functions using deep ReLU neural networks](https://arxiv.org/pdf/1709.05289.pdf)
- [Nonparametric regression using deep neural networks with ReLU activation function](https://arxiv.org/pdf/1708.06633.pdf)

## Lecture 14
- Depth: repeated composition
- Computational graphs
- Machine learning and AI
  * expression power
  * landscape loss surface
  * generalization
  * saddle point
  
- Learning multiple components
- Historical trends: growing data sets
- The MNIST dataset
- Connections per neuron
- Tradeoff(depth/width)
- Solving object recognition
- Iterative optimization
- Curvature
- Direction
- Predicting optimal step size using Taylor series
- Gradient descent and poor conditioning 
- Neural net visualization
- KKR Multipliers
- Cross-entropy
- Tensor

Paper:  
- [The Power of Depth for Feedforward Neural Networks](http://proceedings.mlr.press/v49/eldan16.pdf)
- [Why and When Can Deep-but Not Shallow-networks Avoid the Curse of Dimensionality: A Review](./assets/paper/Why-and-When.pdf)

## Lecture 15 
Paper:
- Deep learning for wireless physical layer: opportunity and challenge decoding 
- 2017 deep learning based MIMO communication 
- An introduction to deep learning for the physical layer 
- Machine learning for wireless network with artificial intelligence
- Deep MIMO detection
- Learning to Optimize : training deep neural networks for wireless resource management
- Quantified advantage of discontinuous weight selection in approximations with deep neural networks
- Error Bound for appointments with deep RELU networks

Paper:
- [Deep Learning-Based MIMO Communications](https://arxiv.org/pdf/1707.07980.pdf)
- [Deep MIMO Detection](https://arxiv.org/pdf/1706.01151.pdf)
- [Deep Learning-Based Communication Over the Air](https://arxiv.org/pdf/1707.03384.pdf)
- [Learning to Optimize: Training Deep Neural Networks for Wireless Resource Management](https://arxiv.org/pdf/1705.09412.pdf)
- [Machine Learning for Wireless Networks with Artificial Intelligence: A Tutorial on Neural Networks](https://arxiv.org/pdf/1710.02913.pdf)
- [Error bounds for approximations with deep ReLU networks](https://arxiv.org/pdf/1610.01145.pdf)
- [Training Feedforward Neural Networks with Standard Logistic Activations is Feasible](https://arxiv.org/pdf/1710.01013.pdf)

## Lecture 16 
- CNN
- RNN 

## Lecture 17
- SVM deep learning 
- Multilayer feed forward networks are universal approximates
- Large scale optimization
- Dropout


## Lecture 18
- Prediction/Inspiration by neuroscience
- Modern statistic theory
- Explosion of computational resources
- Multi-layer perception
- Back-propagation derivation


---
 <h3 id="5">
 Topic 05: Random Matrix Theory for Deep Learning
 </h3>
 
## Lecture 19 
- Free probability random matrix theory
- Limiting spectral density
- Stieltjes transform
- Inverse stieltjes 
- The moment generating function

## Lecture 20 
Paper:
- Exact solutions to the nonlinear dynamics of learning in deep linear neural networks
- Nonlinear random matrix theory for deep learning Euclidean random matrix theory
- Difficult of training deep architecture and effect
- Understanding the difficulty of training deep feed forward neural networks
- Why does unsupervised pre-training help deep learning 
- Big neural networks wastes capacity
- Machine learning basic problem, issue, challenge
- Linear regression
- Hessian vector products
- Convolutions neural network
- Tensoring neural networks
- Tensor deep staking networks
- 2017 a correspondence between random neural networks and statistical field theory

Paper:  
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/pdf/1312.6120.pdf)
- [Supplemental Material: Geometry of Neural Network Loss Surfaces via Random Matrix Theory](http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf)
- [Geometry of Neural Network Loss Surfaces via Random Matrix Theory](http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf)
- [Nonlinear random matrix theory for deep learning](http://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf)
- [Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice](http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice.pdf)

<h3 id="6">
 Topic 06: Tensor for Deep Learning
 </h3>

## Lectrue 21  
Paper:
- A matlab library for stochastic gradient descent algorithm
