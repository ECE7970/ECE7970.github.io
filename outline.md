
### [Topic 01: High Dimensional Space](#1)

### [Topic 02: Best-Fit Subspace and Singular Value Decomposition(SVD)](#2)

### [Topic 03: Random Matrix Theory](#3)

### [Topic 04: Mechine Learning](#4)

### [Topic 05: Deep Learning](#5)

### [Topic 06: Random Matrix Theory in Deep Learning](#6)

---
 <h3 id="1">
 Topic 01: High Dimensional Space
 </h3>
 
## Lectrue 1 
### Big data:
- opportunities and challenges
- High dimensional statistics smart grid
- Cognitive radio network
- Cognitive radar
- Cognitive networked sensing

### Algorithms:
- Random forest
- SVM
- Decision tree
- PCA
- Kernel PCA
   * foundations: deep learning classification, prediction
   * Application


## Lectrue 2 
- High dimension distance
- Supervised learning 
- The law of large number 
- Markov’s inequality
 -Chebyshev’s inequality
 -Law of large numbers
 
 
## Lecture 3 
- Semi-circle distribution
- High dimensional space
- PCA and SVD
- The low of large numbers
- Chebyshev’s inequality
- Distance of variables
- Single ring law
- Tail Bounds theorem
- Geometry of high dimension

## Lecture 4  
- Semi-circle distribution
- CDF and PDF
- Random projection 
- Gaussian annulus theorem and Random projection and Johnson–Lindenstrauss lemma
- Random projection theorem

Paper：  
* [Spectrum Analysis of Large Random Matrices A Brief Introduction](/assets/ppt/SJTU_EE_PhD_Course_Notes_2017.pdf) by Robert C. Qiu
  
## Lecture 5  
- MP law
- Diagonal entries 
- Single-ring distribution  outlier detection anomaly  detection 
- Random projection and Random projection and Johnson–Lindenstrauss lemma

---

## Best-Fit Subspace and Singular Value Decomposition(SVD)
## Lecture 6
- Singular value-decomposition(SVD)
- Singular vectors

## Lecture 7
- Best  fit subspace and singular value decomposition(SVD)
- Best rank-K approximation
- Power method for singular value decomposition
- Applications of singular value decomposition
- PCA
- Clustering

## Lecture 8
- Johnson-Lindertrauss lemma
- Dvoretzky--Milman’s therem     
- Chebyshev’s inequality
- Lindeberg-Lévy Central Limit Theorem
- Concentration sum of independent
- Random vector in high dimension
- Kernel function: kernel PCA

## Lecture 9  
- Kernel function
- Gaussian function
- covariance matrix

---
## Random Matrix Theory
## Lecture 10
- Random matrix theory and big data
- covariance matrix
- Concentration without independence
- Johnson-linderstrauss lemma
- Quadratic forms, summarization and contraction
- Covariance estimation
- Sub Gaussian
- Sample complexity
- Tail bound
- spectral clustering
- Gaussian mixture model

Paper:
- [Sketching as a Tool for Numerical Linear Algebra](http://researcher.watson.ibm.com/researcher/files/us-dpwoodru/wNow.pdf)

---
## Mechine Learning
## Lecture 11
- Introduction to theorem non –asymptotic analysis of random metrics
- Concentration without independent
- Concentration of Lipchitz function on the sphere
- Lipchitz functions
- Concentration via isoperimetric inequalities Gaussian concentration
- Talargand’s concentration inequality
- Random projection
- Sketching
- Random projection
- Correlation coefficient
- Conditional density

---
## Deep Learning
## Lecture 12
- Deep learning
- Why is speech recognition hard?
- The idea about deep learning
- The one  learning algorithm hypothesis
- Neural network
- Deep learning trend
- Bigger is better
- Application
- AI will transform the internet

Paper:  
- [Deep Learning](http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf) by Andrew Ng

## Lecture 13
- Rectified linear unit: deep learning

Paper:  
- [Deep Learning Tutorial](http://speech.ee.ntu.edu.tw/~tlkagk/slide/Deep%20Learning%20Tutorial%20Complete%20(v3)) by Hung-yi Lee
- [Supervised Sequence Labelling with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/preprint.pdf)
- [Deep Learning](./assets/paper/deep-learning-nature.pdf) in nature journal
- [Optimal approximation of piecewise smooth functions using deep ReLU neural networks](https://arxiv.org/pdf/1709.05289.pdf)
- [Nonparametric regression using deep neural networks with ReLU activation function](https://arxiv.org/pdf/1708.06633.pdf)

## Lecture 14
- Depth: repeated composition
- Computational graphs
- Machine learning and AI(expression power, landscape loss surface, generalization, saddle point)
- Learning multiple components
- Historical trends: growing data sets
- The MNIST dataset
- Connections per neuron
- Tradeoff(depth/width
- Solving object recognition
- Iterative optimization
- Curvature
- Direction
- Predicting optimal step size using Taylor series
- Gradient descent and poor conditioning 
- Neural net visualization
- KKR Multipliers
- Softmax function 
- Sigmoid function 
- Cross-entropy
- Tensor

Paper:  
- [The Power of Depth for Feedforward Neural Networks](http://proceedings.mlr.press/v49/eldan16.pdf)
- [Why and When Can Deep-but Not Shallow-networks Avoid the Curse of Dimensionality: A Review](./assets/paper/Why-and-When.pdf)

## Lecture 15 
Paper:
- Deep learning for wireless physical layer: opportunity and challenge decoding 
- 2017 deep learning based MIMO communication 
- An introduction to deep learning for the physical layer 
- Machine learning for wireless network with artificial intelligence
- Deep MIMO detection
- Learning to Optimize : training deep neural networks for wireless resource management
- Quantified advantage of discontinuous weight selection in approximations with deep neural networks
- Error Bound for appointments with deep RELU networks

Paper:
- [Deep Learning-Based MIMO Communications](https://arxiv.org/pdf/1707.07980.pdf)
- [Deep MIMO Detection](https://arxiv.org/pdf/1706.01151.pdf)
- [Deep Learning-Based Communication Over the Air](https://arxiv.org/pdf/1707.03384.pdf)
- [Learning to Optimize: Training Deep Neural Networks for Wireless Resource Management](https://arxiv.org/pdf/1705.09412.pdf)
- [Machine Learning for Wireless Networks with Artificial Intelligence: A Tutorial on Neural Networks](https://arxiv.org/pdf/1710.02913.pdf)
- [Error bounds for approximations with deep ReLU networks](https://arxiv.org/pdf/1610.01145.pdf)
- [Training Feedforward Neural Networks with Standard Logistic Activations is Feasible](https://arxiv.org/pdf/1710.01013.pdf)

## Lecture 16 
- CNN
- RNN 

## Lecture 17
- SVM deep learning 
- Multilayer feed forward networks are universal approximates
- Large scale optimization
- Dropout
- RNN

## Lecture 18
- Prediction/Inspiration by neuroscience
- Modern statistic theory
- Explosion of computational resources
- Multi-layer perception
- Back-propagation derivation
- CNN

## Lecture 19 
- Free probability random matrix theory
- Limiting spectral density
- Stieltjes transform
- Inverse stieltjes 
- The moment generating function

## Lecture 20 
Paper:
- Exact solutions to the nonlinear dynamics of learning in deep linear neural networks
- Nonlinear random matrix theory for deep learning Euclidean random matrix theory
- Difficult of training deep architecture and effect
- Understanding the difficulty of training deep feed forward neural networks
- Why does unsupervised pre-training help deep learning 
- Big neural networks wastes capacity
- Machine learning basic problem, issue, challenge
- Linear regression
- Hessian vector products
- Convolutions neural network
- Tensoring neural networks
- Tensor deep staking networks
- 2017 a correspondence between random neural networks and statistical field theory

Paper:  
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/pdf/1312.6120.pdf)
- [Supplemental Material: Geometry of Neural Network Loss Surfaces via Random Matrix Theory](http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf)
- [Geometry of Neural Network Loss Surfaces via Random Matrix Theory](http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf)
- [Nonlinear random matrix theory for deep learning](http://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf)
- [Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice](http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice.pdf)

## Lectrue 21  
Paper:
- A matlab library for stochastic gradient descent algorithm
