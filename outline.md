# High Dimensional Space

From textbook: Foundations of data science blum. John hoproft and Kaman
## Lectrue 1 
### Big data:
- opportunities and challenges
- High dimensional statistics smart grid
- Cognitive radio network
- Cognitive radar
- Cognitive networked sensing

### Algorithms:
- Random forest
- SVM
- Decision tree
- PCA
- Kernel PCA
   * foundations: deep learning classification, prediction
   * Application


## Lectrue 2 
- High dimension distance
- Supervised learning 
- The law of large number 
- Markov’s inequality
 -Chebyshev’s inequality
 -Law of large numbers
 
 
## Lecture 3 
- Semi-circle distribution
- High dimensional space
- PCA and SVD
- The low of large numbers
- Chebyshev’s inequality
- Distance of variables
- Single ring law
- Tail Bounds theorem
- Geometry of high dimension

## Lecture 4  
- Semi-circle distribution
- CDF and PDF
- Random projection 
- Gaussian annulus theorem and Random projection and Johnson–Lindenstrauss lemma
- Random projection theorem

Paper：  
* [Spectrum Analysis of Large Random Matrices A Brief Introduction](/assets/ppt/SJTU_EE_PhD_Course_Notes_2017.pdf) by Robert C. Qiu
  
## Lecture 5  
- MP law
- Diagonal entries 
- Single-ring distribution  outlier detection anomaly  detection 
- Random projection and Random projection and Johnson–Lindenstrauss lemma

## Lecture 6
- Singular value-decomposition(SVD)
- Singular vectors

## Lecture 7
- Best  fit subspace and singular value decomposition(SVD)
- Best rank-K approximation
- Power method for singular value decomposition
- Applications of singular value decomposition
- PCA
- Clustering

## Lecture 8
- Johnson-Lindertrauss lemma
- Dvoretzky--Milman’s therem     
- Chebyshev’s inequality
- Lindeberg-Lévy Central Limit Theorem
- Concentration sum of independent
- Random vector in high dimension
- Kernel function: kernel PCA

## Lecture 9  
- Kernel function
- Gaussian function
- covariance matrix

## Lecture 10
- Random matrix theory and big data
- covariance matrix
- Concentration without independence
- Johnson-linderstrauss lemma
- Quadratic forms, summarization and contraction
- Covariance estimation
- Sub Gaussian
- Sample complexity
- Tail bound
- spectral clustering
- Gaussian mixture model

Paper:
- [Sketching as a Tool for Numerical Linear Algebra](/assets/paper/2014 Sketching as a Tool for Numerical Linear Algebra.pdf)

## Lecture 11
- Introduction to theorem non –asymptotic analysis of random metrics
- Concentration without independent
- Concentration of Lipchitz function on the sphere
- Lipchitz functions
- Concentration via isoperimetric inequalities Gaussian concentration
- Talargand’s concentration inequality
- Random projection
- Sketching
- Random projection
- Correlation coefficient
- Conditional density


## Lecture 12
- Deep learning
- Why is speech recognition hard?
- The idea about deep learning
- The one  learning algorithm hypothesis
- Neural network
- Deep learning trend
- Bigger is better
- Application
- AI will transform the internet

## Lectrue 13
- Rectified linear unit: deep learning

## Lecture 14
- Depth: repeated composition
- Computational graphs
- Machine learning and AI(expression power, landscape loss surface, generalization, saddle point)
- Learning multiple components
- Historical trends: growing data sets
- The MNIST dataset
- Connections per neuron
- Tradeoff(depth/width
- Solving object recognition
- Iterative optimization
- Curvature
- Direction
- Predicting optimal step size using Taylor series
- Gradient descent and poor conditioning 
- Neural net visualization
- KKR Multipliers
- Softmax function 
- Sigmoid function 
- Cross-entropy
- Tensor

## Lecture 15 
Paper:
- Deep learning for wireless physical layer: opportunity and challenge decoding 
- 2017 deep learning based MIMO communication 
- An introduction to deep learning for the physical layer 
- Machine learning for wireless network with artificial intelligence
- Deep MIMO detection
- Learning to Optimize : training deep neural networks for wireless resource management
- Quantified advantage of discontinuous weight selection in approximations with deep neural networks
- Error Bound for appointments with deep RELU networks


## Lecture 16 
- CNN
- RNN 

## Lecture 17
- SVM deep learning 
- Multilayer feed forward networks are universal approximates
- Large scale optimization
- Dropout
- RNN

## Lecture 18
- Prediction/Inspiration by neuroscience
- Modern statistic theory
- Explosion of computational resources
- Multi-layer perception
- Back-propagation derivation
- CNN

## Lecture 19 
- Free probability random matrix theory
- Limiting spectral density
- Stieltjes transform
- Inverse stieltjes 
- The moment generating function

## Lecture 20 
Paper:
- Exact solutions to the nonlinear dynamics of learning in deep linear neural networks
- Nonlinear random matrix theory for deep learning Euclidean random matrix theory
- Difficult of training deep architecture and effect
- Understanding the difficulty of training deep feed forward neural networks
- Why does unsupervised pre-training help deep learning 
- Big neural networks wastes capacity
- Machine learning basic problem, issue, challenge
- Linear regression
- Hessian vector products
- Convolutions neural network
- Tensoring neural networks
- Tensor deep staking networks
- 2017 a correspondence between random neural networks and statistical field theory

## Lectrue 21  
Paper:
- A matlab library for stochastic gradient descent algorithm
