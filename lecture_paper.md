# Lecture Paper for ECE7970, Fall 2017


### [Topic 01: Deep Learning](#1)

### [Topic 02: Random Matrix Theory for Deep Learning](#2)

### [Topic 03: Tensor for Deep Learning](#3)



---
 <h3 id="1">
 Topic 01: Deep Learning
 </h3>
- [2017 WHY DEEP NEURAL NETWORKS FOR FUNCTION APPROXIMATION](https://openreview.net/pdf?id=SkpSlKIel)
- [2017 Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/pdf/1611.01491.pdf)
- [2017 Understanding deep learning requires rethinking generalization](https://arxiv.org/pdf/1611.03530.pdf)
- [2017 Topology reduction in deep convolutional feature extraction networks](https://arxiv.org/pdf/1707.02711.pdf)
- [2017 Theory of Deep Learning III--Generalization Properties of SGD](https://dspace.mit.edu/bitstream/handle/1721.1/107841/CBMM-Memo-067.pdf?sequence=4)
- [2017 Stable recovery of the factors from a deep matrix product  and application to convolutional network](https://arxiv.org/pdf/1703.08044.pdf)
- [2017 Sharp Minima Can Generalize For Deep Nets](https://arxiv.org/pdf/1703.04933.pdf)
- [2017 Semi-Supervised Learning with the Deep Rendering Mixture Model](https://arxiv.org/pdf/1612.01942.pdf)
- [2017 Scaling the Scattering Transform--Deep Hybrid Networks](https://arxiv.org/pdf/1703.08961.pdf)
- [2017 Robust Large Margin Deep Neural Networks](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934087)    

---

 <h3 id="2">
 Topic 02: Random Matrix Theory for Deep Learning
 </h3>
- [2017 Towards Understanding Generalization of Deep Learning--Perspective of Loss Landscapes](https://arxiv.org/pdf/1706.10239.pdf)
- [2017 Resurrecting the sigmoid in deep learning through dynamical isometry](http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice.pdf)
- [2017 Outliers In The Spectrum for Products of Independent Random Matrices](https://arxiv.org/pdf/1711.07420.pdf)
- [2017 On the Error Exponent of a Random Tensor with  Orthonormal Factor Matrices](https://hal-centralesupelec.archives-ouvertes.fr/hal-01562329/document)
- [2017 Nonlinear random matrix theory for deep learning2](http://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf)
- [2017 Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/pdf/1712.01815.pdf)
- [2017 High-dimensional dynamics of generalization error in neural networks](https://arxiv.org/pdf/1710.03667.pdf)
- [2017 Geometry of Neural Network Loss Surfaces via Random Matrix Theory---supplemental material](http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf)
- [2017 Geometry of Neural Network Loss Surfaces via Random Matrix Theory](http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf)
- [2017 Empirical Analysis of the Hessian of Over-Parametrized Neural Networks](https://arxiv.org/pdf/1706.04454.pdf)
- [2017 Eigenvalues Of The Hession In Deep Learing--Singularity and Beyond](https://openreview.net/pdf?id=B186cP9gx)

---

 <h3 id="3">
 Topic 03: Tensor for Deep Learning
 </h3>
- [2017 Wider and Deeper, Cheaper and Faster:Tensorized LSTMs for Sequence Learning](http://papers.nips.cc/paper/6606-wider-and-deeper-cheaper-and-faster-tensorized-lstms-for-sequence-learning.pdf)
- [2017 Unsupervised and Semi-supervised Anomaly Detection with LSTM Neural Networks](https://arxiv.org/pdf/1710.09207.pdf)
- [2017 Towards the Limit of Network Quantization](https://arxiv.org/pdf/1612.01543.pdf)
- [2017 The Expressive Power of Neural Networks--A View from the Width](http://papers.nips.cc/paper/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.pdf)
- [2017 Tensorizing Generative Adversarial Nets](https://arxiv.org/pdf/1710.10772.pdf)
- [2017 Tensorial Recurrent Neural Networks for Longitudinal Data Analysis](https://arxiv.org/pdf/1708.00185.pdf)
- [2017 Tensor Decomposition via Simultaneous Power Iteration](http://proceedings.mlr.press/v70/wang17i/wang17i.pdf)
- [2017 Tensor Contraction Layers for Parsimonious Deep Nets](https://arxiv.org/pdf/1706.00439.pdf)
- [2017 Tensor Contraction & Regression Networks](https://www.researchgate.net/profile/Jean_Kossaifi2/publication/318721063_Tensor_Contraction_Regression_Networks/links/5a26b48d0f7e9b71dd0c62ad/Tensor-Contraction-Regression-Networks.pdf)
- [2017 Tensor Biclustering](http://papers.nips.cc/paper/6730-tensor-biclustering.pdf)
- [2017 Supervised Learning With Quantum-Inspired Tensor Networks](https://arxiv.org/pdf/1605.05775.pdf)
- [2017 Sharing Residual Units Through Collective Tensor Factorization in Deep Neural Networks](https://arxiv.org/pdf/1703.02180.pdf)
- [2017 Scalable Algorithm for Higher-Order Co-Clustering via Random Sampling](https://arxiv.org/pdf/1703.02180.pdf)
- [2017 Provably efficient neural network representation for image classification](https://arxiv.org/pdf/1703.02180.pdf)
- [2017 Parallelized Tensor Train Learning of Polynomial Classifiers](ftp://ftp.esat.kuleuven.ac.be/pub/pub/stadius/ida/reports/17-05.pdf)

[back](./)
